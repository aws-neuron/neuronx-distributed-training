.. _nxdt_tutorials:

Tutorials
=========

This section will go over a bunch of tutorials to help users get started with NxD Training library.

.. toctree::
    :maxdepth: 1

    Megatron GPT Pretraining <megatron_gpt_pretraining>
    HuggingFace Llama3.1/Llama3-8B Pretraining <hf_llama3_8B_pretraining>
    HuggingFace Llama3.1/Llama3-70B Pretraining <hf_llama3_70B_pretraining>
    HuggingFace Llama3.1/Llama3-8B Supervised Fine-tuning <hf_llama3_8B_SFT>
    HuggingFace Llama3.1/Llama3-8B Efficient Supervised Fine-tuning with LoRA <hf_llama3_8B_SFT_LORA>
    HuggingFace Llama3.1/Llama3-8B Direct Preference Optimization (DPO) and Odds Ratio Preference Optimization (ORPO) based Fine-tuning <hf_llama3_8B_DPO_ORPO>
    Checkpoint Conversion <checkpoint_conversion>
